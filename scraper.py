#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø³ØªØ®Ø±Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ù† Ù…ÙˆÙ‚Ø¹ Mobilltna.org
Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø£ÙˆÙ„ 30 ØµÙØ­Ø© Ù…Ø¹ ØªØ£Ø®ÙŠØ± Ø¢Ù…Ù†
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from urllib.parse import urljoin
import logging
from datetime import datetime

# Ø¥Ø¹Ø¯Ø§Ø¯ Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MobilltnaSScraper:
    def __init__(self):
        self.base_url = "https://mobilltna.org"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.apps_data = {
            "mobile_apps": [],
            "games": [],
            "tv_apps": []
        }
        self.app_id_counter = {
            "mobile_apps": 1,
            "games": 1,
            "tv_apps": 1
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def safe_request(self, url, retries=3):
        """Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø©"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                return response
            except requests.exceptions.RequestException as e:
                logger.warning(f"Ù…Ø­Ø§ÙˆÙ„Ø© {attempt + 1}/{retries} ÙØ´Ù„Øª Ù„Ù„Ù€ URL: {url}")
                if attempt < retries - 1:
                    time.sleep(2 ** attempt)  # ØªØ£Ø®ÙŠØ± ØªØµØ§Ø¹Ø¯ÙŠ
                else:
                    logger.error(f"ÙØ´Ù„ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰: {url}")
                    return None

    def extract_app_icon(self, name):
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙŠÙ‚ÙˆÙ†Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"""
        icons = {
            "player": "fa-video",
            "video": "fa-video",
            "music": "fa-music",
            "photo": "fa-image",
            "camera": "fa-camera",
            "editor": "fa-edit",
            "browse": "fa-globe",
            "browser": "fa-globe",
            "game": "fa-gamepad",
            "puzzle": "fa-puzzle-piece",
            "chess": "fa-chess",
            "sport": "fa-trophy",
            "tv": "fa-tv",
            "movie": "fa-film",
            "series": "fa-theater-masks",
            "book": "fa-book",
            "read": "fa-book",
            "news": "fa-newspaper",
            "weather": "fa-cloud",
            "map": "fa-map",
            "navigate": "fa-map",
            "tool": "fa-tools",
            "util": "fa-wrench",
        }
        
        name_lower = name.lower()
        for keyword, icon in icons.items():
            if keyword in name_lower:
                return icon
        return "fa-star"

    def parse_rating(self, rating_text):
        """ØªØ­Ù„ÙŠÙ„ ØªÙ‚ÙŠÙŠÙ… Ù…Ù† Ø§Ù„Ù†Øµ"""
        if not rating_text:
            return 4.5
        try:
            rating = float(rating_text.split()[0])
            return min(5.0, max(1.0, rating))
        except:
            return 4.5

    def parse_reviews_count(self, reviews_text):
        """ØªØ­Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª"""
        if not reviews_text:
            return 100
        try:
            text = reviews_text.lower().replace(',', '')
            if 'k' in text:
                return int(float(text.replace('k', '')) * 1000)
            return int(text.split()[0])
        except:
            return 100

    def scrape_category(self, category_name, category_url, max_pages=30):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† ÙØ¦Ø© Ù…Ø¹ÙŠÙ†Ø©"""
        logger.info(f"ğŸ”„ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†: {category_name}")
        
        for page in range(1, max_pages + 1):
            try:
                # Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø©
                if page == 1:
                    url = category_url
                else:
                    url = f"{category_url}?page={page}"
                
                logger.info(f"  ğŸ“„ Ø§Ù„ØµÙØ­Ø© {page}/{max_pages}: {url}")
                
                response = self.safe_request(url)
                if not response:
                    logger.warning(f"  âš ï¸ ØªØ®Ø·ÙŠ Ø§Ù„ØµÙØ­Ø© {page}")
                    continue
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ù†Ø§ØµØ± Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª (ÙŠØ®ØªÙ„Ù Ø­Ø³Ø¨ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ÙˆÙ‚Ø¹)
                app_items = soup.find_all('div', class_='app-item')
                
                if not app_items:
                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø­Ø«ÙŠØ© Ø¨Ø¯ÙŠÙ„Ø©
                    app_items = soup.find_all('div', {'data-app': True})
                
                if not app_items:
                    logger.warning(f"  âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚Ø§Øª ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page}")
                    continue
                
                apps_found = 0
                for item in app_items:
                    try:
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                        name_elem = item.find(['h2', 'h3', 'a', 'span'], class_=['app-name', 'title', 'name'])
                        name = name_elem.get_text(strip=True) if name_elem else "Unknown App"
                        
                        desc_elem = item.find(['p', 'span'], class_=['description', 'desc', 'summary'])
                        description = desc_elem.get_text(strip=True) if desc_elem else "ØªØ·Ø¨ÙŠÙ‚ Ù…Ù…ÙŠØ²"
                        
                        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙˆØµÙ Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹ØŒ Ù‚ØµØ±Ù‡
                        if len(description) > 150:
                            description = description[:150] + "..."
                        
                        rating_elem = item.find(['span', 'div'], class_=['rating', 'rate', 'stars'])
                        rating = self.parse_rating(rating_elem.get_text() if rating_elem else None)
                        
                        reviews_elem = item.find(['span', 'div'], class_=['reviews', 'count', 'votes'])
                        reviews = self.parse_reviews_count(reviews_elem.get_text() if reviews_elem else None)
                        
                        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ¦Ø©
                        if "game" in category_name.lower() or "Ù„Ø¹Ø¨Ø©" in category_name.lower():
                            cat_key = "games"
                        elif "tv" in category_name.lower() or "ØªÙ„ÙØ§Ø²" in category_name.lower():
                            cat_key = "tv_apps"
                        else:
                            cat_key = "mobile_apps"
                        
                        app_obj = {
                            "id": self.app_id_counter[cat_key],
                            "name": name,
                            "description": description,
                            "icon": self.extract_app_icon(name),
                            "rating": rating,
                            "reviews": reviews,
                            "category": "app",
                            "download_link": f"https://mobilltna.org/app/{name.replace(' ', '-').lower()}"
                        }
                        
                        self.apps_data[cat_key].append(app_obj)
                        self.app_id_counter[cat_key] += 1
                        apps_found += 1
                        
                    except Exception as e:
                        logger.debug(f"    âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ ØªØ·Ø¨ÙŠÙ‚: {e}")
                        continue
                
                logger.info(f"  âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {apps_found} ØªØ·Ø¨ÙŠÙ‚ Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page}")
                
                # ØªØ£Ø®ÙŠØ± Ø¢Ù…Ù† Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
                if page < max_pages:
                    time.sleep(1.5)
                
            except Exception as e:
                logger.error(f"  âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø© {page}: {e}")
                continue
        
        logger.info(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ {category_name}: {len(self.apps_data[cat_key])} ØªØ·Ø¨ÙŠÙ‚")

    def scrape_all(self):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª"""
        logger.info("=" * 60)
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ù† Mobilltna.org")
        logger.info("=" * 60)
        
        categories = [
            ("ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…ÙˆØ¨Ø§ÙŠÙ„", f"{self.base_url}/apps", "mobile_apps"),
            ("Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨", f"{self.base_url}/games", "games"),
            ("ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙ„ÙØ§Ø²", f"{self.base_url}/tv", "tv_apps"),
        ]
        
        for cat_name, cat_url, cat_key in categories:
            self.scrape_category(cat_name, cat_url, max_pages=30)
            time.sleep(2)  # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„ÙØ¦Ø§Øª
        
        return self.apps_data

    def save_to_json(self, filename='data/apps_new.json'):
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ù„Ù JSON"""
        try:
            filepath = filename
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.apps_data, f, ensure_ascii=False, indent=2)
            
            total = (len(self.apps_data["mobile_apps"]) + 
                    len(self.apps_data["games"]) + 
                    len(self.apps_data["tv_apps"]))
            
            logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ: {filepath}")
            logger.info(f"ğŸ“Š Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total} ØªØ·Ø¨ÙŠÙ‚")
            logger.info(f"   â€¢ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…ÙˆØ¨Ø§ÙŠÙ„: {len(self.apps_data['mobile_apps'])}")
            logger.info(f"   â€¢ Ø£Ù„Ø¹Ø§Ø¨: {len(self.apps_data['games'])}")
            logger.info(f"   â€¢ ØªØ·Ø¨ÙŠÙ‚Ø§Øª TV: {len(self.apps_data['tv_apps'])}")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù: {e}")

if __name__ == "__main__":
    try:
        logger.info(f"â° Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {datetime.now()}")
        
        scraper = MobilltnaSScraper()
        data = scraper.scrape_all()
        scraper.save_to_json('data/apps_new.json')
        
        logger.info(f"â° Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬: {datetime.now()}")
        logger.info("=" * 60)
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}", exc_info=True)